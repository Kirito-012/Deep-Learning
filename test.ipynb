{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\11ukn\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'tuner'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m34\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Learning rate finder\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m lr_find_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuner\u001b[49m\u001b[38;5;241m.\u001b[39mlr_find(\n\u001b[0;32m     52\u001b[0m     model,\n\u001b[0;32m     53\u001b[0m     train_dataloaders\u001b[38;5;241m=\u001b[39mdataloader,\n\u001b[0;32m     54\u001b[0m     min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m     55\u001b[0m     max_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,\n\u001b[0;32m     56\u001b[0m     early_stop_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Get suggestion\u001b[39;00m\n\u001b[0;32m     60\u001b[0m new_lr \u001b[38;5;241m=\u001b[39m lr_find_results\u001b[38;5;241m.\u001b[39msuggestion()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'tuner'"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import SGD\n",
    "\n",
    "class BasicLigthningTrain(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w00 = nn.Parameter(torch.tensor(1.7), requires_grad=False)\n",
    "        self.b00 = nn.Parameter(torch.tensor(-0.85), requires_grad=False)\n",
    "        self.w01 = nn.Parameter(torch.tensor(-40.8), requires_grad=False)\n",
    "        self.w10 = nn.Parameter(torch.tensor(12.6), requires_grad=False)\n",
    "        self.b10 = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.w11 = nn.Parameter(torch.tensor(2.7), requires_grad=False)\n",
    "        self.final_bias = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        self.learning_rate = 0.01\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input_to_top_relu = input * self.w00 + self.b00\n",
    "        top_relu_output = F.relu(input_to_top_relu)\n",
    "        scaled_top_relu = self.w01 * top_relu_output\n",
    "        input_to_bottom_relu = input * self.w10 + self.b10\n",
    "        bottom_relu_output = F.relu(input_to_bottom_relu)\n",
    "        scaled_bottom_relu = self.w11 * bottom_relu_output\n",
    "        input_to_final_relu = scaled_bottom_relu + scaled_top_relu + self.final_bias\n",
    "        output = F.relu(input_to_final_relu)\n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return SGD(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i)\n",
    "        loss = (output_i - label_i) ** 2\n",
    "        return loss\n",
    "\n",
    "# Data setup\n",
    "inputs = torch.tensor([0., 0.5, 1.0])\n",
    "labels = torch.tensor([0.0, 1.0, 0.0])\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "# Model and Trainer\n",
    "model = BasicLigthningTrain()\n",
    "trainer = L.Trainer(max_epochs=34)\n",
    "\n",
    "# Learning rate finder\n",
    "lr_find_results = trainer.tuner.lr_find(\n",
    "    model,\n",
    "    train_dataloaders=dataloader,\n",
    "    min_lr=0.001,\n",
    "    max_lr=1.0,\n",
    "    early_stop_threshold=None\n",
    ")\n",
    "\n",
    "# Get suggestion\n",
    "new_lr = lr_find_results.suggestion()\n",
    "print(f\"lr_find() suggests {new_lr:.5f} for the learning rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
