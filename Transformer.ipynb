{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {'what' : 0,\n",
    "                'is' : 1,\n",
    "                'StatQuest' : 2,\n",
    "                'Awesome' : 3, \n",
    "                '<EOS>':4\n",
    "            }\n",
    "id_to_token = dict(map(reversed, token_to_id.items()))\n",
    "\n",
    "inputs = torch.tensor([[token_to_id['what'],\n",
    "                        token_to_id['is'],\n",
    "                        token_to_id['StatQuest'],\n",
    "                        token_to_id['<EOS>'],\n",
    "                        token_to_id['Awesome']],\n",
    "\n",
    "                        [token_to_id['StatQuest'],\n",
    "                        token_to_id['is'],\n",
    "                        token_to_id['what'],\n",
    "                        token_to_id['<EOS>'],\n",
    "                        token_to_id['Awesome']]])\n",
    " \n",
    "labels = torch.tensor([[token_to_id['is'],\n",
    "                        token_to_id['StatQuest'],\n",
    "                        token_to_id['<EOS>'],\n",
    "                        token_to_id['Awesome'],\n",
    "                        token_to_id['<EOS>']],\n",
    "\n",
    "                        [token_to_id['is'],\n",
    "                        token_to_id['what'],\n",
    "                        token_to_id['<EOS>'],\n",
    "                        token_to_id['Awesome'],\n",
    "                        token_to_id['<EOS>']]])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PE(pos,2i) = sin(pos/1000^(2i/d_model))\n",
    "\n",
    "d_model = number of embeddings for each token\n",
    "\n",
    "pos = position of the token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # max_len is the maximum number of tokens our Transformer can process. \n",
    "    # In real life, you would set them to much much larger values\n",
    "    def __init__(self, max_len = 6  , d_model = 2):\n",
    "        super().__init__()\n",
    "        # (6,2)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    " \n",
    "        # position = [[0.],[1.],[2.],[3.],[4.],[5.]]\n",
    "        position = torch.arange(0, max_len, 1).float().unsqueeze(1) \n",
    "        # embedding = [0.]\n",
    "        embedding = torch.arange(0, d_model, 2).float()\n",
    "\n",
    "        div_term = 1/torch.tensor(1000) ** (embedding / d_model)\n",
    "\n",
    "        # div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        return word_embeddings + self.pe[:word_embeddings.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention (Q, K, V) = SoftMax ((Q.K^T)/sqrt(d_k) + M) V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False) # Shape = (6,2)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False) # Shape = (6,2)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False) # Shape = (6,2)\n",
    "\n",
    "        # sequence_length\n",
    "        self.row_dim = 0\n",
    "        # embedding_size\n",
    "        self.col_dim = 1\n",
    "\n",
    "    def forward(self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
    "        q = self.W_q(encodings_for_q) # Shape = (6,2)\n",
    "        k = self.W_q(encodings_for_k) # Shape = (6,2) \n",
    "        v = self.W_q(encodings_for_v) # Shape = (6,2)\n",
    "\n",
    "        # Multiplying Query^T and Key (sims -> similarity_score)\n",
    "        # q -> Shape = (6,2)\n",
    "        # k.transpose -> Shape = (2,6)\n",
    "        # sims -> Shape = (6,6) representing similarity scores b/w all pairs of query and key\n",
    "        sims = torch.matmul(q, k.transpose(dim0 = self.row_dim, dim1 = self.col_dim))\n",
    "\n",
    "        # k.size(self.col_dim) -> gonna return the dimnsion of column, that is, 2\n",
    "        # scaling the sims\n",
    "        # Shape = (6,6)\n",
    "        scaled_sims = sims/torch.tensor(k.size(self.col_dim)**0.5)\n",
    "\n",
    "        # for autoregressiveness\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "        # For scaled_sims = (4,4)\n",
    "        # scaled_sims = [[ 1.0,  0.5,  0.8,  0.2],\n",
    "        #               [ 0.3,  1.2, -0.1,  0.9],\n",
    "        #               [ 0.7,  0.4,  1.5, -0.3],\n",
    "        #               [-0.2,  0.6,  0.1,  1.1]]\n",
    "        \n",
    "        # mask = [[False,  True,  True,  True],\n",
    "        #        [False, False,  True,  True],\n",
    "        #        [False, False, False,  True],\n",
    "        #        [False, False, False, False]] \n",
    "\n",
    "        # scaled_sims = [[ 1.0, -1e9, -1e9, -1e9],\n",
    "        #               [ 0.3,  1.2, -1e9, -1e9],\n",
    "        #               [ 0.7,  0.4,  1.5, -1e9],\n",
    "        #               [-0.2,  0.6,  0.1,  1.1]]\n",
    "\n",
    "        # dim = self.col_dim -> means along axis 1 that is row, that is [ 1.0, -1e9, -1e9, -1e9]\n",
    "        attention_percent = F.softmax(scaled_sims, dim=self.col_dim)\n",
    "\n",
    "        # attention_percent = [[1.000, 0.000, 0.000, 0.000],\n",
    "        #                     [0.289, 0.711, 0.000, 0.000],\n",
    "        #                     [0.252, 0.187, 0.561, 0.000],\n",
    "        #                     [0.121, 0.270, 0.164, 0.445]]\n",
    "\n",
    "        # attention_percent -> Shape = (4,4)\n",
    "        # v -> Shape = (4,2) for the example\n",
    "        # v =  [[2.0, 1.0],    # Value for token 0\n",
    "        #       [0.5, 3.0],    # Value for token 1\n",
    "        #       [1.5, -1.0],   # Value for token 2\n",
    "        #       [-0.5, 2.5]]   # Value for token 3\n",
    "        # attention_score -> Shape = (4,2)\n",
    "        attention_scores = torch.matmul(attention_percent, v)\n",
    "        # attention_scores = [[2.0000, 1.0000],\n",
    "        #                    [0.9335, 2.4220],\n",
    "        #                    [1.4390, 0.2520],\n",
    "        #                    [0.4005, 1.8795]]\n",
    "        # 4 attention scores for 4 inputs\n",
    "        \n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(L.LightningModule):\n",
    "    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
    "        super().__init__()\n",
    "\n",
    "        # forms an Embedding layer \n",
    "        # num_embeddings = how many rows the lookup table should have\n",
    "        # embedding_dim = number of values we want to represent each token (if 2 -> token 0 would look like [x,y])\n",
    "\n",
    "        # Weights initialized by Embedding Layer\n",
    "        # self.we.weight = [[0.5, -0.2],   # Token 0\n",
    "        #                   [1.0,  0.8],   # Token 1\n",
    "        #                   [-0.3, 0.4],   # Token 2\n",
    "        #                   [0.1, -0.5]]   # Token 3\n",
    "        # word_embeddings & self.we = [[-0.3,  0.4], # Position 0 (Token 2)\n",
    "        #                             [1.0,   0.8],  # Position 1 (Token 1)\n",
    "        #                             [0.5,  -0.2],  # Position 2 (Token 0)\n",
    "        #                             [0.1,  -0.5],  # Position 3 (Token 3)\n",
    "        #                             [1.0,   0.8],  # Position 4 (Token 1)\n",
    "        #                             [0.5,  -0.2]]  # Position 5 (Token 0)\n",
    "\n",
    "        self.we = nn.Embedding(num_embeddings=num_tokens, embedding_dim=d_model) # Shape = (6,2)\n",
    "\n",
    "        self.pe = PositionalEncoding(d_model=d_model, max_len=max_len) # Shape = (6,2)\n",
    "        # positional encoding basially gives the sin, cos values, precomputed values\n",
    "        # self.pe.pe = [[ 0.0000,  1.0000],  # Position 0\n",
    "        #           [ 0.8415,  0.5403],  # Position 1\n",
    "        #           [ 0.9093, -0.4161],  # Position 2\n",
    "        #           [ 0.1411, -0.9899],  # Position 3\n",
    "        #           [-0.7568, -0.6536],  # Position 4\n",
    "        #           [-0.9589,  0.2837]]  # Position 5\n",
    "        # in PositionalEncoding() -> word_embeddings.size(0) will be 6 for this given scenario \n",
    "        # \n",
    "        # self.pe = position_encoded = word_embeddings(self.we) + self.pe.pe\n",
    "        # # self.pe = [[-0.3 + 0.0000,  0.4 + 1.0000],     ---->     [[-0.3000,  1.4000]\n",
    "        #             [1.0 + 0.8415,   0.8 + 0.5403],      ---->     [ 1.8415,  1.3403]\n",
    "        #             [0.5 + 0.9093,  -0.2 + -0.4161],     ---->     [ 1.4093, -0.6161]\n",
    "        #             [0.1 + 0.1411,  -0.5 + -0.9899],     ---->     [ 0.2411, -1.4899]\n",
    "        #             [1.0 + -0.7568,  0.8 + -0.6536],     ---->     [ 0.2432,  0.1464]\n",
    "        #             [0.5 + -0.9589, -0.2 + 0.2837]]      ---->     [-0.4589,  0.0837]] \n",
    "\n",
    "        \n",
    "        self.self_attention = Attention(d_model=d_model) # Shape = (6,2)\n",
    "        # self.self_attention = [[0.4000,  1.5500],\n",
    "        #                       [1.5613,  0.9282],\n",
    "        #                       [1.1958,  0.1599],\n",
    "        #                       [0.1468, -0.5879],\n",
    "        #                       [0.3528,  0.0940],\n",
    "        #                       [0.1087,  0.0877]]\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        # inputs = 2(integer value for each taken), outputs = 4(all possible outputs)\n",
    "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, token_ids):\n",
    "        word_embeddings = self.we(token_ids)\n",
    "        position_encoded = self.pe(word_embeddings)\n",
    "\n",
    "        # tril -> lower triangle\n",
    "        # token_ids = no. of tokens passed (let's say = 4)\n",
    "        # torch.ones((token_ids.size(dim=0), token_ids.size(dim=0))) -> creates a matrix of ones of (4,4)\n",
    "        # tril -> leaves the values in the lower triangle as they are, and turns everything else as 0's\n",
    "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0))))\n",
    "        # mask = [[1.,0.,0.,0.],\n",
    "        #         [1.,1.,0.,0.],\n",
    "        #         [1.,1.,1.,0.],\n",
    "        #         [1.,1.,1.,1.]]\n",
    "\n",
    "        mask = mask == 0\n",
    "        # mask = [[False, True,  True,  True],\n",
    "        #         [False, False, True,  True],\n",
    "        #         [False, False, False, True],\n",
    "        #         [False, False, False, False]]\n",
    "\n",
    "        # position encoded 3 times -> for key, value, query\n",
    "        # Attention (Q, K, V) = SoftMax ((Q.K^T)/sqrt(d^k) + M) V\n",
    "        # this formula is applied and stuff\n",
    "        self_attention_values = self.self_attention(position_encoded, position_encoded, position_encoded, mask=mask)\n",
    "        residual_connection_values = position_encoded + self_attention_values\n",
    "        \n",
    "        # output of the fully connected layer \n",
    "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
    "\n",
    "        return fc_layer_output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=0.1)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_tokens, labels= batch\n",
    "        output_i = self.forward(input_tokens[0])\n",
    "        loss = self.loss(output_i, labels[0])\n",
    "        return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "model = DecoderOnlyTransformer(num_tokens=len(token_to_id), d_model=2, max_len=6)\n",
    "model_input = torch.tensor([\n",
    "                        token_to_id['what'],\n",
    "                        token_to_id['is'],\n",
    "                        token_to_id['StatQuest'],\n",
    "                        token_to_id['<EOS>']\n",
    "                    ])\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "max_length = 6\n",
    "\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    print('\\t', id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name           | Type               | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | we             | Embedding          | 10     | train\n",
      "1 | pe             | PositionalEncoding | 0      | train\n",
      "2 | self_attention | Attention          | 12     | train\n",
      "3 | fc_layer       | Linear             | 15     | train\n",
      "4 | loss           | CrossEntropyLoss   | 0      | train\n",
      "--------------------------------------------------------------\n",
      "37        Trainable params\n",
      "0         Non-trainable params\n",
      "37        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "8         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:   0%|          | 0/2 [00:00<?, ?it/s, v_num=0]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 2/2 [00:00<00:00, 72.16it/s, v_num=0] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 2/2 [00:00<00:00, 48.89it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=30)\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Tokens:\n",
      "\n",
      "\t Awesome\n",
      "\t <EOS>\n"
     ]
    }
   ],
   "source": [
    "model_input = torch.tensor([\n",
    "                        token_to_id['what'],\n",
    "                        token_to_id['is'],\n",
    "                        token_to_id['StatQuest'],\n",
    "                        token_to_id['<EOS>']\n",
    "                    ])\n",
    "\n",
    "input_length = model_input.size(dim=0)\n",
    "predictions = model(model_input)\n",
    "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "predicted_ids = predicted_id\n",
    "\n",
    "max_length = 6\n",
    "\n",
    "for i in range(input_length, max_length):\n",
    "    if (predicted_id == token_to_id[\"<EOS>\"]):\n",
    "        break\n",
    "    model_input = torch.cat((model_input, predicted_id))\n",
    "    predictions = model(model_input)\n",
    "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
    "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
    "\n",
    "print(\"Predicted Tokens:\\n\")\n",
    "for id in predicted_ids:\n",
    "    print('\\t', id_to_token[id.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
