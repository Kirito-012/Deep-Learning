{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLightningTrain(L.LightningModule):\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w00 = nn.Parameter(torch.tensor(1.7), requires_grad=False)\n",
    "        self.b00 = nn.Parameter(torch.tensor(-0.85), requires_grad=False)\n",
    "        self.w01 = nn.Parameter(torch.tensor(-40.8), requires_grad=False)\n",
    "\n",
    "        self.w10 = nn.Parameter(torch.tensor(12.6), requires_grad=False)\n",
    "        self.b10 = nn.Parameter(torch.tensor(0.0), requires_grad=False)\n",
    "        self.w11 = nn.Parameter(torch.tensor(2.7), requires_grad=False)\n",
    "\n",
    "        self.final_bias = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_to_top_relu = self.w00 * input + self.b00\n",
    "        top_relu_output = F.relu(input_to_top_relu)\n",
    "        scaled_top_relu_output = self.w01 * top_relu_output\n",
    "\n",
    "        input_to_bottom_relu = self.w10 * input + self.b10\n",
    "        bottom_relu_output = F.relu(input_to_bottom_relu)\n",
    "        scaled_bottom_relu_output = self.w11 * bottom_relu_output\n",
    "\n",
    "        input_to_final_relu = self.final_bias + scaled_bottom_relu_output + scaled_top_relu_output\n",
    "\n",
    "        output = F.relu(input_to_final_relu)\n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # self.parameters() returns the values that can be trained, i.e. where the requires_grad = True\n",
    "        # basically we are returning -------> \"Use SGD to tweak final_bias with a step size of 0.01 to minimize the loss.\"\n",
    "        return SGD(self.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "    # batch -> chunk of data provided by the DataLoader\n",
    "    # input_i, output_i will be assigned the value which is given to the batch \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # What is a batch, anyway?\n",
    "        # Think of it like a small group of examples the model learns from at once\n",
    "        input_i, label_i = batch\n",
    "        # taking the input and doing the math using forward function and storing the output for that input in the output_i \n",
    "        output_i = self.forward(input_i)\n",
    "        # predicted_output - actual_output ** 2\n",
    "        loss = (output_i - label_i) ** 2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([0., 0.5, 1.0])\n",
    "labels = torch.tensor([0.0, 1.0, 0.0])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "\n",
    "# this is what a batch uses in the training_step function\n",
    "# dataloader basically have the whole pair of inputs, labels, i.e dataset, and it would serve one batch at a time to the model for training\n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:00<00:00, 341.45it/s]`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:00<00:00, 346.25it/s]\n",
      "Learning rate set to 0.016595869074375606\n",
      "Restoring states from the checkpoint path at c:\\Users\\11ukn\\OneDrive\\Desktop\\Project ASUNA\\Projects (ML)\\Deep Learning - Duality\\.lr_find_a57e926c-ee32-4932-87c2-8ab4f7aa1ac4.ckpt\n",
      "Restored all states from the checkpoint at c:\\Users\\11ukn\\OneDrive\\Desktop\\Project ASUNA\\Projects (ML)\\Deep Learning - Duality\\.lr_find_a57e926c-ee32-4932-87c2-8ab4f7aa1ac4.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016595869074375606\n"
     ]
    }
   ],
   "source": [
    "model = BasicLightningTrain()\n",
    "\n",
    "# Trainer --> responsible for running the training, testing, or tuning of your model. It handles all the complicated stuff—like looping over data, calling the optimizer, and tracking progress\n",
    "trainer = L.Trainer(max_epochs=100)\n",
    "\n",
    "tuner = L.pytorch.tuner.Tuner(trainer)\n",
    "\n",
    "lr_finder = tuner.lr_find(model, train_dataloaders = dataloader, min_lr = 0.01, max_lr = 1.0, early_stop_threshold=None)\n",
    "\n",
    "lr_result = lr_finder.suggestion()\n",
    "print(lr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 7      | n/a \n",
      "---------------------------------------------\n",
      "1         Trainable params\n",
      "6         Non-trainable params\n",
      "7         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 3/3 [00:00<00:00, 204.86it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 3/3 [00:00<00:00, 129.52it/s, v_num=2]\n"
     ]
    }
   ],
   "source": [
    "model.learning_rate = lr_result\n",
    "trainer.fit(model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-15.4624)\n"
     ]
    }
   ],
   "source": [
    "print(model.final_bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
