{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('input.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 42, 55, 55, 54]\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "# Word Embedding\n",
    "vocab_size = len(set(text))\n",
    "string_to_integers = {char:i for i,char in enumerate(set(text))}\n",
    "integers_to_string = {i:char for i,char in enumerate(set(text))}\n",
    "\n",
    "encode = lambda x: [string_to_integers[char] for char in x]\n",
    "decode = lambda x: ''.join([integers_to_string[i] for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 12, 57,  ...,  2, 17, 16])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "data = torch.tensor(encode(text))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.9 * len(data))\n",
    "train_data, val_data = data[:split], data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1, 42, 55, 45, 39, 16,  4, 42],\n",
       "         [15, 40, 50,  3, 15, 53, 58, 40],\n",
       "         [40, 50, 15, 50, 12,  1, 15, 52],\n",
       "         [50, 58, 55, 55, 15, 53, 42, 15]]),\n",
       " tensor([[42, 55, 45, 39, 16,  4, 42, 40],\n",
       "         [40, 50,  3, 15, 53, 58, 40, 40],\n",
       "         [50, 15, 50, 12,  1, 15, 52, 50],\n",
       "         [58, 55, 55, 15, 53, 42, 15, 54]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "block_size = 8\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "\n",
    "    x = torch.stack([data[i : i+block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1 : i+block_size+1] for i in idx])\n",
    "    return x, y\n",
    "\n",
    "xb,yb = get_batch(\"train\")\n",
    "xb, yb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits --> torch.Size([32, 65])\n",
      "Loss --> tensor(4.2446)\n",
      "\n",
      "Generated Text --> YlIhbjU3ux\n",
      "QgFVLIjIdwzPeLIlXk:NS.wmjMx:''R:$TE,Mw-DACoelQ,QCoTBWZbEs?pmIE'ha.-au?GULI,h;NaXN.\n",
      "Q&wXpEk\n"
     ]
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None): \n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "        \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = model(xb, yb)\n",
    "print(\"Logits -->\", logits.shape)\n",
    "print(\"Loss -->\", loss.data)\n",
    "\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "print('\\nGenerated Text -->',decode(model.generate(idx, 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at 0 step: 2.4603488445281982\n",
      "Loss at 1000 step: 2.461993455886841\n",
      "Loss at 2000 step: 2.468071699142456\n",
      "Loss at 3000 step: 2.438849687576294\n",
      "Loss at 4000 step: 2.7277064323425293\n",
      "Loss at 5000 step: 2.525634765625\n",
      "Loss at 6000 step: 2.4847307205200195\n",
      "Loss at 7000 step: 2.4815597534179688\n",
      "Loss at 8000 step: 2.513474702835083\n",
      "Loss at 9000 step: 2.43355131149292\n",
      "Loss at 10000 step: 2.547548532485962\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(10001):\n",
    "    xb,yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Loss at {i} step: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text --> YOLOLAns k ond t bo prerts cuno-me MELIS:\n",
      "\n",
      "NELL: mive, won hacame I d wnyoy.\n",
      "Te mem thoong G meyor?\n",
      "Thanestoorarg tlow t htis moorere? baskeind in t es, ES:\n",
      "Fore hest by, mespef we.\n",
      "Bokse. y llittrgan mpen spumafficand ailinowine, tsunonarssbrt me an ns ter he thencongory more Hat oto h, eathod; intheeanounat I iso t m Je.\n",
      "I mor thiracunghithyspotit GHOf' a mereyer ovore\n",
      "d gule,\n",
      "MICapotes s bent\n",
      "Pomy by ire,\n",
      "Th ry 's.\n",
      "\n",
      "INouralis ce the indy bauncthelk my, he wer\n",
      "Thakioro VIst fad o, r w, ct.\n",
      "My't ofed ss ve nd II:\n",
      "IOFote vespe, maindrf the g aterdell m tomasen vitonou f t therore w lasar?\n",
      "taint s CUMEL:\n",
      "\n",
      "ARor g wese BE atof plelealrt dy NCUCK:\n",
      "TABr.\n",
      "\n",
      "AR:\n",
      "Lane, y momy umanoowand, ONCE a m th, ad thet, are my,\n",
      "Thatomeano,-br.\n",
      "\n",
      "Fatht, batouof atharsay.\n",
      "t\n",
      "\n",
      "GLEOut awe.\n",
      "GA enererveid dstharond wns t nds, mbormyours st qunds t iave s;\n",
      "Ay:\n",
      "SO:\n",
      "Hos h toug rinon ghacaletind air veacr be\n",
      "ABYos ethee'seeast whio y il, y ais veno ter chan d sexe thathondinld athy, pre fall irowape hacott, chath t pr\n"
     ]
    }
   ],
   "source": [
    "print('\\nGenerated Text -->',decode(model.generate(idx, 1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_iters = 200\n",
    "@torch.no_grad()\n",
    "\n",
    "def estimateloss():\n",
    "    output = {}\n",
    "    for split in ('train', 'val'):\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        output[split] = losses.mean()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embed = 32\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(n_embed, head_size, bias=False) \n",
    "        self.k = nn.Linear(n_embed, head_size, bias=False) \n",
    "        self.v = nn.Linear(n_embed, head_size, bias=False) \n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        weight = q @ k.transpose(-2,-1)/ (C**0.5)\n",
    "        weight = weight.masked_fill(self.tril[:T,:T] == 0, float('-inf'))\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        v = self.v(x)\n",
    "        output = weight @ v\n",
    "        return output\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, train loss: 4.144351959228516, val loss: 4.1462721824646\n",
      "iter: 500, train loss: 2.670701026916504, val loss: 2.6704397201538086\n",
      "iter: 1000, train loss: 2.5157458782196045, val loss: 2.5134007930755615\n",
      "iter: 1500, train loss: 2.465735912322998, val loss: 2.488044261932373\n",
      "iter: 2000, train loss: 2.4485418796539307, val loss: 2.450505256652832\n",
      "iter: 2500, train loss: 2.417445421218872, val loss: 2.4401233196258545\n",
      "iter: 3000, train loss: 2.4144177436828613, val loss: 2.4353299140930176\n",
      "iter: 3500, train loss: 2.3961381912231445, val loss: 2.410214424133301\n",
      "iter: 4000, train loss: 2.38267183303833, val loss: 2.4129457473754883\n",
      "iter: 4500, train loss: 2.3866395950317383, val loss: 2.406050205230713\n",
      "iter: 5000, train loss: 2.384495973587036, val loss: 2.3933897018432617\n",
      "iter: 5500, train loss: 2.3681063652038574, val loss: 2.398341655731201\n",
      "iter: 6000, train loss: 2.382218599319458, val loss: 2.3885114192962646\n",
      "\n",
      "Generated Text -->\n",
      "Yos,\n",
      "Bousr yeee owit-3-hy orof se panto chind ste wi or buly,\n",
      "Ans we ndongew inlin hrevel tilsatu lowwheicoly det, haiche, yod kyoused YOnd uith ted how bes:\n",
      "Whovead ld ad?\n",
      "\n",
      "GLO:\n",
      "In, LI arccerbse;\n",
      "And thou t palrergorot aris ofur graru.\n",
      "\n",
      "sacerepre ilofl yo thas the des fa the in nt cath Lincolet favet ds;\n",
      "ONou tatnowwe in an a astarea-kor spor'scck he ait!\n",
      "Ben bue hotome:\n",
      "We.\n",
      "\n",
      "LAs hord,\n",
      "TIUS: anetake pishe coure beailernoasay\n",
      "H yom ale arit thentg:\n",
      "I\n",
      "OUS:\n",
      "Sy!\n",
      "he atoubly tabrran. Fhir geas ooutn th were!\n",
      "Tour thintesr whay, wholl.\n",
      "\n",
      "US:\n",
      "With I dsans eese youglon EXTELYerodot\n",
      "JUNTenyend blu th fant rmar hises.\n",
      "Whake?\n",
      "An ti, atch cas\n",
      "brolout hinot ppi teinefon rsehy cien'ndo se st the bepe\n",
      "Rt.\n",
      "\n",
      "Mee tlol meaneatrierg e's whe tusimean tino; ntant pr obouy mown dy wpeaith siefa itorud arg?\n",
      "CIOLUCERELLE:\n",
      "I porubed okik the, fo beard ROSCHALERNTis ful os; wir mo yoofle wheredave toru-$arse thourde,\n",
      "Sreavelid hand wout yesnoe oucos othomyouspean bit, sons the,\n",
      "Whe ben wrebe, by? ithun. ONHAnge an\n"
     ]
    }
   ],
   "source": [
    "# Using Self - Attention\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_head = Head(n_embed)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None): \n",
    "        B,T = idx.shape \n",
    "        token_embed = self.token_embedding_table(idx)\n",
    "        position_embed = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_embed + position_embed\n",
    "        x = self.sa_head(x)\n",
    "    \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "        \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for iter in range(6001):\n",
    "    if iter % 500 == 0:\n",
    "        losses = estimateloss()   \n",
    "        print(f\"iter: {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "    \n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long)\n",
    "print('\\nGenerated Text -->')\n",
    "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, train loss: 4.264521598815918, val loss: 4.258363723754883\n",
      "iter: 500, train loss: 2.6452715396881104, val loss: 2.6466567516326904\n",
      "iter: 1000, train loss: 2.4621636867523193, val loss: 2.456190586090088\n",
      "iter: 1500, train loss: 2.3988797664642334, val loss: 2.4059195518493652\n",
      "iter: 2000, train loss: 2.3525640964508057, val loss: 2.371034860610962\n",
      "iter: 2500, train loss: 2.3161797523498535, val loss: 2.334740400314331\n",
      "iter: 3000, train loss: 2.294144630432129, val loss: 2.316192388534546\n",
      "iter: 3500, train loss: 2.271512031555176, val loss: 2.2918736934661865\n",
      "iter: 4000, train loss: 2.258976697921753, val loss: 2.286926031112671\n",
      "iter: 4500, train loss: 2.2420597076416016, val loss: 2.290806770324707\n",
      "iter: 5000, train loss: 2.2474329471588135, val loss: 2.2719106674194336\n",
      "iter: 5500, train loss: 2.235119581222534, val loss: 2.2514097690582275\n",
      "iter: 6000, train loss: 2.2345893383026123, val loss: 2.265702724456787\n",
      "\n",
      "Generated Text -->\n",
      "YGlor he ist, the heple, the beffor onve,\n",
      "Thar nus boow ilf, butt monm'thin, weeveve:\n",
      "God Ond my mp's\n",
      "FarqRunt lend sence send bet weelied thardik henecth Jue nesto thus to myou ut all thamtnk wence's thur,\n",
      "And's ilet be his he by? 't puso, houle:\n",
      "If Ongrce Minextw my;\n",
      "Lir vord, ieef: youdry dy.\n",
      "\n",
      "Lot your I mant thell que be aret, I'f di,\n",
      "Ditng thy bret aker,\n",
      "Eo I not uchmer moo neat, kigor manit yorole lexed 'llid kim brome heaits\n",
      "Whand ibcc's Mland\n",
      "\n",
      "Hin D I atin, 'Thee forve athe chiell, by the fie teeingecty, whe heall omer noof; chy leatarthy semy mer pir'y grese the you hense wech bream deperbseat and Dy O cin nowe O theres, condin in lathat ace of'laime thar:\n",
      "My stow\n",
      "Pagus rith trom\n",
      "bone:\n",
      "'plow'd pie\n",
      "QUkhan handto athill nou, cris\n",
      "Dred you if a it boe ther thed go my ploour dep.\n",
      "QUCES:\n",
      "Nooth Se?\n",
      "\n",
      "JIUKSE MOL whe fou forss youmaduervionged dever fesis:\n",
      "Weall lace whave thited.\n",
      "\n",
      "Norjard thin haing, rosme:\n",
      "Yod, Wicland Wher aken do la, ringced wen pount lameast Loollll conh Rose the, \n"
     ]
    }
   ],
   "source": [
    "# Using MultiHead Attention\n",
    "# Using Self - Attention\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.sa_head = MultiHeadAttention(4, n_embed//4)\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None): \n",
    "        B,T = idx.shape \n",
    "        token_embed = self.token_embedding_table(idx)\n",
    "        position_embed = self.position_embedding_table(torch.arange(T))\n",
    "        x = token_embed + position_embed\n",
    "        x = self.sa_head(x)\n",
    "    \n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:,-1,:]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "        \n",
    "model = BigramLanguageModel(vocab_size)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for iter in range(6001):\n",
    "    if iter % 500 == 0:\n",
    "        losses = estimateloss()   \n",
    "        print(f\"iter: {iter}, train loss: {losses['train']}, val loss: {losses['val']}\")\n",
    "    \n",
    "    xb, yb = get_batch(\"train\")\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long)\n",
    "print('\\nGenerated Text -->')\n",
    "print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
